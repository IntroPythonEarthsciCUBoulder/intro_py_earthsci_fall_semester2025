{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85fc1ecd-176f-4f61-ba99-5a29a57eb266",
   "metadata": {},
   "source": [
    "# Assignment 4: Vegetation and the Landscape\n",
    "\n",
    "(YOUR NAME HERE)\n",
    "\n",
    "(TODAY'S DATE HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d4f941-e67e-4b67-80bb-d7f48a7695fe",
   "metadata": {},
   "source": [
    "*In solving each of the problems below, please include text comments and description for your future self, so that when you look back you'll have notes on how you solved these problems!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1137c683-cc0b-4abe-aa68-d16d4aca04db",
   "metadata": {},
   "source": [
    "# Background: Raster Bands and NDVI\n",
    "In this assignment you will investigate how different topographic metrics correspond to vegetation.  To do so, we will calculate the **normalized difference vegetation index** (**NDVI**) in an area around Boulder.  Plant leaves do not absorb light in the near-infrared part (NIR)  of the electromagnetic spectrum.  In contrast [chlorophyll is most effecient at capturing red light](https://en.wikipedia.org/wiki/Photosynthetically_active_radiation).  This means if you were to look at the radiation signature of an area with lots of plants, it would be emititng lots of NIR and little red light.   From this idea we get [NDVI](https://en.wikipedia.org/wiki/Normalized_difference_vegetation_index):\n",
    "\n",
    "$$ \\text{NDVI}=\\frac{(\\text{NIR}-\\text{Red})}{(\\text{NIR}+\\text{Red})}$$\n",
    "\n",
    "[Landsat](https://en.wikipedia.org/wiki/Landsat_program) is a series of satellites that have been circling the earth since the early 70s.  The sensors on the Landsat sattelites collect (among other things) grids of the ammount of Blue, Green, and Red light (what your phone camera collects!) and NIR radiation reflected from the surface of the Earth.  We call each set of the spectrum captured a \"band\" of data.  So the \"blue band\" of a Landsat gird is grid of numbers that reflects the ammount of blue light reflected at each point on the surface.  We can use this to calculate NDVI for every cell of a Landsat grid.  Each cell of a Ldndsat is like a pixel in a photograph, but it corresponds to a sectio of the surface of the earth.\n",
    "\n",
    "There is more detail on rasters, imagary, and light in the appendix at the bottom of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9044396a-d80d-4bf5-948c-f68cdc0217c0",
   "metadata": {},
   "source": [
    "# Part 1: Reading in data and making maps\n",
    "Along with this notebook there are the follwing files:\n",
    "* `mountain_blue.tif` - the blue band of our landsat grid\n",
    "* `mountain_green.tif`- the green band of our landsat grid\n",
    "* `mountain_red.tif` - the red band of our landsat grid\n",
    "* `mountain_nir.tif` - the NIR band of our landsat grid\n",
    "* `mountain_dem.tif` - A DEM of the same area covered by the Landsat grid\n",
    "All data files have the UTM zone 13N coordinate system.  Which means that units are in meters.  Each pixel is 30 meters wide.\n",
    "  \n",
    "*Write a function that will take a landsat band as input, and make an appropriately labeled image plot of that band.  Run that for all 4 Landsat bands.  Note that if you want to run your function using  a loop, you may have trouble displaying all plots.  You are encouraged the consult the web if you would like to go this route.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae548ca3-2a5a-4b39-8823-3374ed3cd356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9b66f04-7b51-4b43-a284-5a824fece233",
   "metadata": {},
   "source": [
    "# Part 2: Calculating NDVI\n",
    "*Write a function that takes numpy arrays representing the red and NIR bands of a Landsat grid, and returns a new array representing a map the NDVI.  Run this function and make a plot of the resulting NDVI*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6437a87e-d61f-4a2d-acd6-5c6f72c85172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77235c49-3773-4ae3-9d33-66042ce11aa2",
   "metadata": {},
   "source": [
    "# Part 3: Calculating slope\n",
    "*Write a program that takes the DEM as an input parameter and calculates a slope array.  Make a plot of the slope array*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad3ea87-27db-4014-8f1c-1b7b28bbe28e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c587d2a8-5e4e-4a6f-9b61-b044e0a2b494",
   "metadata": {},
   "source": [
    "# Part 4: Relationships between datasets\n",
    "Now we have an array that represents vegation (NDVI) as well as elevation and slope.  \n",
    "*Write one-two sentances on how you think NDVI will relate to elevation and slope.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870e79de-bb4b-40b6-b0f3-59501f3b27f9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "caaab0a2-0138-4bea-8cf7-d150aa76d005",
   "metadata": {},
   "source": [
    "*Write a program that uses Matplotlib to make labeled scatter plots of how NDVI varries with elevation and slope.  Hint: every array represents a 30m by 30m square of land and each array covers the same ammount of area.  What does that mean about the first entry of every array?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f35394-f9f6-483c-80f0-9d1765f1ff44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a521031d-5bd9-43d5-8806-52eb2cf57d8b",
   "metadata": {},
   "source": [
    "# Part 5: Aspect (for graduate students, or as a bonus point)\n",
    "In notebook 14 there were other ways to process DEMs that we didn't get to.  We will use one of these: Aspect.\n",
    "\n",
    "*Write a program that calculates aspect for your DEM, and makes a plot of how NDVI varries with aspect*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ed576c-4074-4430-aae4-c880e8a63054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a78f1855-e8b9-4345-9abd-8435cc306f22",
   "metadata": {},
   "source": [
    "# Part 6: Outliers (for graduate students, or as a bonus point)\n",
    "You may have noticed that there are some outlier NDVI values which are impacting your plot.  This could be related to water, which usually has a very high NDVI value (why might that be?).  If this is water, than those values aren't telling you anything about vegetation, so it would be nice to exclude.  One way we can do so is with a **masked array** in `numpy`.  A masked array is a numpy array with the value blanked or masked out at certain parts.  You can typically use them just like arrays but the masked values will be ignored.  \n",
    "\n",
    "*Take a look at the documentation for the functions in the masked array module in numpy: https://numpy.org/doc/stable/reference/routines.ma.html\n",
    "Find a function that will generate a masked array from your NDVI array with values above an outlier value you determine masked out.  Redo your plots from parts 4 and 5 with this masked array.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aae90d-6590-48a5-ac2a-dc48d3b14e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6783335a-5ce1-4fa7-8f9f-2096b6379121",
   "metadata": {},
   "source": [
    "# Part 7: Maps (bonus point for all)\n",
    "Now that we have a few different data grids, we can combine them together and make maps.  It turns out that if we call `plt.imshow()` twice in a row, it plots the two images on top of each other.  We can exploit that by making our second image transparent.  This is done with the `alpha` parameter of the `imshow` function.  We can also set the boundaries of our plot so that they are meaningful and unitful instead of rows and columns.  We can do this by passing a list of values as the `extent` parameter when we call `imshow`.\n",
    "\n",
    "*In the code cell below, use the provided function to create a hillshade from the DEM.  Then plot that and a semitransparent NDVI image together.  Set the extent based on the actual location of our data.  Finally, label the axes appropriately.  You will need to know that the latitude and longitude of the top left is (40.2449,-105.6885) and the latitude and longitude of the bottom right is (39.9578,-105.3314).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5474ac-c881-4713-8339-3a79007754a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hillshade(z, azimuth=315.0, angle_altitude=45.0):\n",
    "    \"\"\"Generate a hillshade image from DEM.\n",
    "\n",
    "    Notes: adapted from example on GeoExamples blog,\n",
    "    published March 24, 2014, by Roger Veciana i Rovira.\n",
    "    \"\"\"\n",
    "    x, y = np.gradient(z)\n",
    "    slope = np.pi / 2.0 - np.arctan(np.sqrt(x**2 + y**2))  # slope gradient\n",
    "    aspect = np.arctan2(-x, y)  # aspect\n",
    "    azimuthrad = azimuth * np.pi / 180.0  # convert lighting azimuth to radians\n",
    "    altituderad = angle_altitude * np.pi / 180.0  # convert lighting altitude to radians\n",
    "    shaded = np.sin(altituderad) * np.sin(slope) + np.cos(altituderad) * np.cos(\n",
    "        slope\n",
    "    ) * np.cos(azimuthrad - aspect)\n",
    "    return 255 * (shaded + 1) / 2  # return result scaled 0 to 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1497d0a-209b-4f6b-b024-a642250d5ac8",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32f8595-7486-4837-bc87-18f32474b65f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# How Computers Represent Photographs\n",
    "An photograph is a recording of light reflected off something (typically refered to as a scene).  When a human observes something visually, it is light reflected off objects that activates specialized cells in the retina.  Photographs work the same way, using a variety of technologies to substitute for the cells of the retina.  Traditional film photography uses a film full of light-sensitive silver salt crystal, part of which becomes metallic silver when exposed to light.  Digital photography uses an array of photosensitive circuits that record the ammount of light.  In either case, what you are left with is (typically but not necessarily) a rectangle that represents the amount of light reflected from a captured scene.\n",
    "## Rasters\n",
    "The result of a digital image is called a \"raster\" an array of values representing the ammount of light at each position.  Below is a sample raster repreented in python as a list of lists of integers.  The first list is the upper row of pixels in our image.  The top left pixel has a value of 255, meaning very bright.  The bottom right has a value of 60, rather dark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880bba72-1d3d-4110-9ce2-f8090b6088ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = [[255,200,180,180,180],\n",
    "         [200,200,100,100,100],\n",
    "         [180,180,80, 100, 80],\n",
    "         [170,170,70, 100, 60],\n",
    "         [160,160,60, 100, 60]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d66cf3-d3c9-477f-9c70-22c44f257ef0",
   "metadata": {},
   "source": [
    "A typical pixel in a computer image will have values that range from 0 to 255.  Think about what we learned about how computers represent numbers.  What does this mean about the underlying data structure of a pixel?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41164c4e-b17f-4c40-9215-06901812437e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66e43f87-4f6a-48eb-8b9e-7e4785b77d11",
   "metadata": {},
   "source": [
    "In python we can view images with a library called \"matplotlib\".  Matplotlib can do lots of things including make many different types of graphs.  Now we'll use the imshow function to display our image above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfa255d-bdcc-41cb-aeab-8995c3bd91d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(image, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa8dede-4cdf-49bd-a3a9-ae98945971dc",
   "metadata": {},
   "source": [
    "Now, try to make you own \"image\" using a list of lists, and display it with matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd1da48-fb95-4c66-b8d1-b54413e0325f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b24b353a-abb5-4285-a2d9-c34fb82c4847",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Color photography\n",
    "The sensors (silver salt or digitial circuit) record the *ammount* of light that hit them, not necessarily the *color* of light.  So how do we get color images?  Humans see color because specific types of cells in the retina are sensitive to specific wavelengths of light.  Humans are sensitive to red, green, and blue light (see below).\n",
    "![human color sensitivity](https://upload.wikimedia.org/wikipedia/commons/f/f1/1416_Color_Sensitivity.svg)\n",
    "Our eyes take in the ammount of red, green, and blue light in what we see, and construct color based on that.  We can mimic that with a digital image by recording the ammount of red, green, and blue light.  This can be done by filtering out different types of light, but you can think of a digital camera as, like an eye, having three different types of sensors.  When a computer displays an image, rather than each pixel representing a certain ammount of light, like in our example above, each \"pixel\" is three lights, one red, one green, and one blue.  Each light as a brightness that corresponds to the relative ammount of that color light in that part of the scene.  Put your phone screen under a microscope sometime, you'll see that it's actually made up of a bunch of red, green,. and blue lights!\n",
    "### Color rasters\n",
    "But how do we store this on a computer?  Whereas before, we had a list of lists, or an array where each value was the ammount of light reflected from a scene, instead we could have each value be a tuple contaning the ammount of red, green, and blue light!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5129f3fd-7345-49a7-aec2-25f8f9826b62",
   "metadata": {},
   "source": [
    "pixel = (120, 0, 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953a270e-ee5a-4176-acd4-30b937913437",
   "metadata": {},
   "source": [
    "This would lead us to an image that would be something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a91d6c-38ca-4e6f-b7a7-d3fad308e58b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "color_image = [[(120, 0, 120), (120, 10, 120), (120, 20, 120)],\n",
    "               [(130, 0, 120), (120, 10, 130), (130, 20, 130)],\n",
    "               [(140, 0, 120), (130, 10, 130), (140, 20, 140)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08173c7-1a7d-4326-823d-4b41db74d241",
   "metadata": {
    "tags": []
   },
   "source": [
    "Typically, for reasons that aren't important now, instead of images being stored as 2d arrays of tuples of 3, they are stored as 3 2d arrays, each array representing a different type of light, often called a \"band\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b31032-e7d5-4d76-8d96-ce55c186788d",
   "metadata": {},
   "source": [
    "## From images to spectral data\n",
    "Sure humans typically only distinguish between red, green, and blue light, but we can build sensors to distinguish between all sorts of different parts of the electromagnetic spectrum.\n",
    "![the electromagnetic spectrum](https://upload.wikimedia.org/wikipedia/commons/thumb/2/25/Electromagnetic-Spectrum.svg/1024px-Electromagnetic-Spectrum.svg.png)\n",
    "A common \"band\" of the spectrum of interest to a wide range of scientists is the \"infrared\" section, netween visible light and radio waves.  This section of the spectrum can capture thermal radiation, like the heat off of a body (ever seen a picture from \"night vision\" goggles?\" and has a wide range of applications, including vegetation health.\n",
    "\n",
    "The satellite (Landsat 8) that captured the satellite image near Boulder contains a few bands beyond the visible spectrum.  We've included the near infrared band, but read about the Landsat satelite and other bands it includes.  Do you have any ideas about how you could use other bands?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a93ea1e-2a46-432a-a180-62fb01b16ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
